"""
ä¸»ç®¡é“ç¨‹åº - Member 6
åŠŸèƒ½ï¼šæ•´åˆæ‰€æœ‰æ¨¡å—ï¼Œæ‰§è¡Œå®Œæ•´çš„æ•°æ®å¤„ç†æµç¨‹
"""

import json
import os
import logging
from datetime import datetime
import sys


# æ·»åŠ  src ç›®å½•åˆ°è·¯å¾„
sys.path.insert(
    0, os.path.join(os.path.dirname(os.path.dirname(__file__)), "src")
)  # ../../src

# é…ç½®æ—¥å¿—
log_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), "logs")  # ../../logs
os.makedirs(log_dir, exist_ok=True)

# é…ç½®æ–‡ä»¶å¤„ç†å™¨ï¼ˆUTF-8ç¼–ç ï¼‰
file_handler = logging.FileHandler(
    os.path.join(log_dir, "pipeline.log"), encoding="utf-8"
)
file_handler.setLevel(logging.INFO)
file_handler.setFormatter(
    logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
)

# é…ç½®æ§åˆ¶å°å¤„ç†å™¨ï¼ˆUTF-8ç¼–ç ï¼‰
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
console_handler.setFormatter(
    logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
)
# è®¾ç½®æ§åˆ¶å°è¾“å‡ºç¼–ç ä¸ºUTF-8
if sys.platform == "win32":
    import codecs

    sys.stdout = codecs.getwriter("utf-8")(sys.stdout.buffer, "ignore")
    sys.stderr = codecs.getwriter("utf-8")(sys.stderr.buffer, "ignore")

logging.basicConfig(level=logging.INFO, handlers=[file_handler, console_handler])
logger = logging.getLogger(__name__)


class Pipeline:
    """ä¸»ç®¡é“ç±»"""

    def __init__(self, config_path: str | None = None):
        """åˆå§‹åŒ–ç®¡é“"""
        if config_path is None:
            config_path = os.path.join(
                os.path.dirname(os.path.dirname(__file__)), "config", "config.json"
            )  # ../../config/config.json
        self.config_path = config_path
        self.load_config()
        self.create_directories()

    def load_config(self):
        """åŠ è½½é…ç½®"""
        try:
            with open(self.config_path, "r", encoding="utf-8") as f:
                self.config = json.load(f)
            logger.info("é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
        except Exception as e:
            logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            sys.exit(1)

    def create_directories(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
        directories = [
            self.config["output"]["data_dir"],
            self.config["output"]["temp_dir"],
            os.path.dirname(self.config["output"]["html_file"]),
        ]

        for directory in directories:
            os.makedirs(directory, exist_ok=True)
            logger.info(f"ç›®å½•å·²åˆ›å»º/ç¡®è®¤: {directory}")

    def step0_topic_acquisition(self):
        """Gain topic from user input"""

        self.topic = input(
            "Please enter the topic or event you want to search for: "
        ).strip()

        logger.info(f"âœ“ æ­¥éª¤0å®Œæˆ: ä¸»é¢˜å·²è·å– - {self.topic}")
        return True

    def step1_data_acquisition(self):
        """æ­¥éª¤1: çˆ¬å–æ•°æ®"""
        logger.info("=" * 50)
        logger.info("æ­¥éª¤1: å¼€å§‹çˆ¬å–æ•°æ®")
        logger.info("=" * 50)

        scraper = None
        try:
            # # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ topic æœç´¢ç»“æœ
            # import glob
            # topic_files = glob.glob(os.path.join(self.config['output']['data_dir'], 'topic_*.json'))

            # if topic_files:
            #     # ä½¿ç”¨æœ€æ–°çš„ topic æœç´¢ç»“æœ
            #     latest_topic_file = max(topic_files, key=os.path.getmtime)
            #     logger.info(f"å‘ç°ä¸»é¢˜æœç´¢ç»“æœ: {os.path.basename(latest_topic_file)}")
            #     logger.info(f"å°†ä½¿ç”¨è¯¥æ–‡ä»¶çš„æ•°æ®ï¼Œè·³è¿‡é»˜è®¤æºçˆ¬å–")

            #     with open(latest_topic_file, 'r', encoding='utf-8') as f:
            #         data = json.load(f)
            #         # topicæ–‡ä»¶å¯èƒ½æœ‰ä¸¤ç§æ ¼å¼ï¼šåˆ—è¡¨æˆ–åŒ…è£…å¯¹è±¡
            #         if isinstance(data, list):
            #             articles = data
            #         elif isinstance(data, dict) and 'articles' in data:
            #             articles = data['articles']
            #         else:
            #             articles = data

            # # ä¿å­˜åˆ° raw_articles.json ä¾›åç»­æ­¥éª¤ä½¿ç”¨
            # raw_path = os.path.join(self.config['output']['data_dir'], 'raw_articles.json')
            # with open(raw_path, 'w', encoding='utf-8') as f:
            #     json.dump(articles, f, ensure_ascii=False, indent=2)

            # logger.info(f"âœ“ æ­¥éª¤1å®Œæˆ: ä½¿ç”¨äº† {len(articles)} ç¯‡ä¸»é¢˜æœç´¢æ–‡ç« ")
            # return True

            # å¦‚æœæ²¡æœ‰ topic æ–‡ä»¶ï¼ŒæŒ‰åŸæ¥çš„æ–¹å¼çˆ¬å–
            # from scraper import MyNewsScraper

            # scraper = MyNewsScraper(self.config_path, use_database=True)
            # articles = scraper.scrape_all_sources()

            # if not articles:
            #     logger.warning("æœªçˆ¬å–åˆ°ä»»ä½•æ–‡ç« ï¼Œä½¿ç”¨ç¤ºä¾‹æ•°æ®")
            #     articles = self._create_sample_data()

            # # ä¿å­˜åˆ°æ–‡ä»¶å’Œæ•°æ®åº“
            # scraper.save_articles(articles)

            from atss.news_source import RssMetaNewsSource, MetaNewsSource, News

            def news_to_article(news: News):
                article = news.to_dict()
                article["published_date"] = article.pop("published_at", "")
                return article

            def acuire_news_from_rss():
                from atss.search_engine.fts import FTSSearchEngine
                from atss.db_utils import ArticleStorage

                # load opml file
                opml_config = self.config["datasource"]["rss"]["opml"]
                opml_paths = []
                for c in opml_config:
                    if c["enabled"]:
                        opml_path = c["file"]
                        opml_paths.append(opml_path)

                news_sources = []
                for opml_path in opml_paths:
                    with open(opml_path, "r", encoding="utf-8") as f:
                        opml_content = f.read()
                    news_source = RssMetaNewsSource(opml_content)
                    news_sources.append(news_source)

                news_source = MetaNewsSource(news_sources)

                storage = ArticleStorage(reset=False)
                # storage the news from RSS
                storage.save_articles(
                    list(
                        map(lambda news: news_to_article(news), news_source.get_news())
                    )
                )

                # search the news by topic
                search_engine = FTSSearchEngine()
                news = list(search_engine.search(self.topic))
                return news

            def acuire_news_from_websearch():
                from atss.search_engine.webscraper import WebScraperSearchEngine
                from atss.config import get_config

                webscraper_config = get_config()["search_engine"]["webscraper"]

                # search the news using web search

                search_engines = {
                    name: WebScraperSearchEngine(cfg["sitemap"], limit=10)
                    for name, cfg in webscraper_config.items()
                    if cfg["enabled"]
                }

                all_news = []
                for name, search_engine in search_engines.items():
                    logger.info(f"Searching news with Web Search Engine {name}...")
                    all_news.extend(list(search_engine.search(self.topic)))
                return all_news

            rss_news = acuire_news_from_rss()
            websearch_news = acuire_news_from_websearch()

            news = rss_news + websearch_news

            # Save articles to files # !NOTE: just for compatibility

            from atss.scraper import MyNewsScraper

            scraper = MyNewsScraper(self.config_path, use_database=True)
            articles = [news_to_article(n) for n in news]
            scraper._save_to_file(articles)

            logger.info(f"âœ“ æ­¥éª¤1å®Œæˆ: çˆ¬å–äº† {len(articles)} ç¯‡æ–‡ç« ")
            return True

        except Exception as e:
            logger.error(f"âœ— æ­¥éª¤1å¤±è´¥: {e}", stack_info=True)
            # åˆ›å»ºç¤ºä¾‹æ•°æ®ä»¥ä¾¿ç»§ç»­æµç¨‹
            articles = self._create_sample_data()
            from atss.scraper import MyNewsScraper

            scraper_fallback = MyNewsScraper(self.config_path, use_database=False)
            scraper_fallback._save_to_file(articles)
            logger.info("ä½¿ç”¨ç¤ºä¾‹æ•°æ®ç»§ç»­")
            return True
        finally:
            if scraper:
                scraper.close()

    def step2_clean_data(self):
        """æ­¥éª¤2: æ¸…æ´—æ•°æ®"""
        logger.info("=" * 50)
        logger.info("æ­¥éª¤2: å¼€å§‹æ¸…æ´—æ•°æ®")
        logger.info("=" * 50)

        try:
            from atss.data_cleaner import DataCleaner

            # è¯»å–åŸå§‹æ•°æ®
            with open("data/raw_articles.json", "r", encoding="utf-8") as f:
                raw_articles = json.load(f)

            cleaner = DataCleaner()
            cleaned_articles = cleaner.clean_dataset(raw_articles)
            cleaner.save_cleaned_data(cleaned_articles)

            logger.info(f"âœ“ æ­¥éª¤2å®Œæˆ: æ¸…æ´—äº† {len(cleaned_articles)} ç¯‡æ–‡ç« ")
            return True

        except Exception as e:
            logger.error(f"âœ— æ­¥éª¤2å¤±è´¥: {e}")
            return False

    def step3_deduplicate(self):
        """æ­¥éª¤3: å»é‡"""
        logger.info("=" * 50)
        logger.info("æ­¥éª¤3: å¼€å§‹å»é‡")
        logger.info("=" * 50)

        try:
            from atss.deduplicator import Deduplicator

            with open("data/cleaned_articles.json", "r", encoding="utf-8") as f:
                cleaned_articles = json.load(f)

            deduplicator = Deduplicator(
                similarity_threshold=self.config["deduplication"][
                    "similarity_threshold"
                ]
            )
            deduplicated_articles = deduplicator.deduplicate(cleaned_articles)
            deduplicator.save_deduplicated_data(deduplicated_articles)

            logger.info(f"âœ“ æ­¥éª¤3å®Œæˆ: å»é‡åå‰©ä½™ {len(deduplicated_articles)} ç¯‡æ–‡ç« ")
            return True

        except Exception as e:
            logger.error(f"âœ— æ­¥éª¤3å¤±è´¥: {e}")
            return False

    def step4_extract_entities(self):
        """æ­¥éª¤4: æå–å®ä½“"""
        logger.info("=" * 50)
        logger.info("æ­¥éª¤4: å¼€å§‹æå–å®ä½“")
        logger.info("=" * 50)

        try:
            from atss.entity_extractor import EntityExtractor

            with open("data/deduplicated_articles.json", "r", encoding="utf-8") as f:
                articles = json.load(f)

            extractor = EntityExtractor(self.config_path)
            entities_data = extractor.extract_and_rank(articles)
            extractor.save_entities(entities_data)

            logger.info(f"âœ“ æ­¥éª¤4å®Œæˆ: æå–äº†å®ä½“")
            return True

        except Exception as e:
            logger.error(f"âœ— æ­¥éª¤4å¤±è´¥: {e}")
            return False

    def step5_generate_summary(self):
        """æ­¥éª¤5: ç”Ÿæˆæ‘˜è¦"""
        logger.info("=" * 50)
        logger.info("æ­¥éª¤5: å¼€å§‹ç”Ÿæˆæ‘˜è¦")
        logger.info("=" * 50)

        try:
            from atss.summarizer import Summarizer

            with open("data/deduplicated_articles.json", "r", encoding="utf-8") as f:
                articles = json.load(f)

            summarizer = Summarizer(self.config_path)
            summary = summarizer.generate_summary(articles)
            summarizer.save_summary(summary)

            logger.info(f"âœ“ æ­¥éª¤5å®Œæˆ: ç”Ÿæˆäº†æ‘˜è¦")
            return True

        except Exception as e:
            logger.error(f"âœ— æ­¥éª¤5å¤±è´¥: {e}")
            return False

    def step6_generate_timeline(self):
        """æ­¥éª¤6: ç”Ÿæˆæ—¶é—´çº¿"""
        logger.info("=" * 50)
        logger.info("æ­¥éª¤6: å¼€å§‹ç”Ÿæˆæ—¶é—´çº¿")
        logger.info("=" * 50)

        try:
            from atss.timeline_generator import TimelineGenerator

            with open("data/deduplicated_articles.json", "r", encoding="utf-8") as f:
                articles = json.load(f)

            generator = TimelineGenerator()
            timeline = generator.create_timeline(articles)
            generator.save_timeline(timeline)

            logger.info(f"âœ“ æ­¥éª¤6å®Œæˆ: ç”Ÿæˆäº† {len(timeline)} ä¸ªæ—¶é—´èŠ‚ç‚¹")
            return True

        except Exception as e:
            logger.error(f"âœ— æ­¥éª¤6å¤±è´¥: {e}")
            return False

    def step7_generate_html(self):
        """æ­¥éª¤7: ç”ŸæˆHTMLé¡µé¢"""
        logger.info("=" * 50)
        logger.info("æ­¥éª¤7: å¼€å§‹ç”ŸæˆHTMLé¡µé¢")
        logger.info("=" * 50)

        try:
            from atss.html_generator import HTMLGenerator

            # # è·å–ä¸»é¢˜åç§°ï¼ˆä»topicæ–‡ä»¶åæˆ–æ‘˜è¦ä¸­æå–ï¼‰
            # topic_name = None
            # import glob
            # topic_files = glob.glob(os.path.join(self.config['output']['data_dir'], 'topic_*.json'))
            # if topic_files:
            #     latest_topic_file = max(topic_files, key=os.path.getmtime)
            #     # ä»æ–‡ä»¶åæå–ä¸»é¢˜ï¼štopic_ä¸»é¢˜å_æ—¶é—´æˆ³.json
            #     filename = os.path.basename(latest_topic_file)
            #     parts = filename.replace('.json', '').split('_')
            #     if len(parts) >= 2:
            #         topic_name = '_'.join(parts[1:-1])  # å»æ‰'topic'å’Œæ—¶é—´æˆ³
            topic_name = self.topic

            generator = HTMLGenerator(self.config_path)
            data = generator.load_data()
            html_content = generator.generate_html(data)
            actual_file = generator.save_html(html_content, topic_name=topic_name)

            # ä¿å­˜å®é™…ç”Ÿæˆçš„æ–‡ä»¶è·¯å¾„ä¾›åç»­ä½¿ç”¨
            self.generated_html_file = actual_file

            logger.info(f"âœ“ æ­¥éª¤7å®Œæˆ: HTMLé¡µé¢å·²ç”Ÿæˆ")
            return True

        except Exception as e:
            logger.error(f"âœ— æ­¥éª¤7å¤±è´¥: {e}")
            return False

    def _create_sample_data(self):
        """åˆ›å»ºç¤ºä¾‹æ•°æ®"""
        sample_articles = [
            {
                "title": "AI Technology Breakthrough in Natural Language Processing",
                "content": "Researchers have announced a major breakthrough in natural language processing technology. The new model shows unprecedented accuracy in understanding context and generating human-like responses. This development is expected to revolutionize various industries including customer service, content creation, and education. The team behind this achievement includes leading scientists from multiple institutions.",
                "url": "https://example.com/article1",
                "source": "Tech News",
                "published_date": "2025-11-15",
                "scraped_at": datetime.now().isoformat(),
            },
            {
                "title": "New Climate Agreement Reached at International Summit",
                "content": "World leaders have reached a historic climate agreement at the international summit. The agreement includes commitments to reduce carbon emissions by 50% by 2030. Major countries including the United States, China, and European nations have pledged significant investments in renewable energy. Environmental organizations have praised the agreement as a crucial step forward.",
                "url": "https://example.com/article2",
                "source": "World News",
                "published_date": "2025-11-16",
                "scraped_at": datetime.now().isoformat(),
            },
            {
                "title": "Tech Giant Announces Revolutionary Quantum Computer",
                "content": "A major technology company has unveiled its latest quantum computer, claiming it can solve complex problems thousands of times faster than traditional computers. The breakthrough has implications for drug discovery, financial modeling, and cryptography. Industry experts believe this marks the beginning of the quantum computing era.",
                "url": "https://example.com/article3",
                "source": "Innovation Daily",
                "published_date": "2025-11-17",
                "scraped_at": datetime.now().isoformat(),
            },
        ]

        logger.info("å·²åˆ›å»ºç¤ºä¾‹æ•°æ®ç”¨äºæ¼”ç¤º")
        return sample_articles

    def run(self):
        """è¿è¡Œå®Œæ•´ç®¡é“"""
        logger.info("\n" + "=" * 60)
        logger.info("å¼€å§‹æ‰§è¡Œè‡ªåŠ¨åŒ–ä¸»é¢˜æ‘˜è¦ç”Ÿæˆç®¡é“")
        logger.info("=" * 60 + "\n")

        start_time = datetime.now()

        steps = [
            ("è·å–topic", self.step0_topic_acquisition),
            ("çˆ¬å–æ•°æ®", self.step1_data_acquisition),
            ("æ¸…æ´—æ•°æ®", self.step2_clean_data),
            ("å»é‡å¤„ç†", self.step3_deduplicate),
            ("æå–å®ä½“", self.step4_extract_entities),
            ("ç”Ÿæˆæ‘˜è¦", self.step5_generate_summary),
            ("ç”Ÿæˆæ—¶é—´çº¿", self.step6_generate_timeline),
            ("ç”ŸæˆHTML", self.step7_generate_html),
        ]

        results = []
        for step_name, step_func in steps:
            try:
                success = step_func()
                results.append((step_name, success))

                if not success:
                    logger.warning(f"æ­¥éª¤ '{step_name}' å¤±è´¥ï¼Œä½†ç»§ç»­æ‰§è¡Œ")

            except Exception as e:
                logger.error(f"æ­¥éª¤ '{step_name}' å‘ç”Ÿå¼‚å¸¸: {e}")
                results.append((step_name, False))

        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        # æ‰“å°æ€»ç»“
        logger.info("\n" + "=" * 60)
        logger.info("ç®¡é“æ‰§è¡Œå®Œæˆ")
        logger.info("=" * 60)
        logger.info(f"æ€»è€—æ—¶: {duration:.2f} ç§’")
        logger.info("\næ‰§è¡Œç»“æœ:")

        for step_name, success in results:
            status = "âœ“" if success else "âœ—"
            logger.info(f"  {status} {step_name}")

        # ç»Ÿè®¡
        success_count = sum(1 for _, success in results if success)
        logger.info(f"\næˆåŠŸ: {success_count}/{len(results)} ä¸ªæ­¥éª¤")

        if success_count == len(results):
            logger.info("\nğŸ‰ æ‰€æœ‰æ­¥éª¤æˆåŠŸå®Œæˆï¼")
            # æ˜¾ç¤ºå®é™…ç”Ÿæˆçš„HTMLæ–‡ä»¶è·¯å¾„
            html_file = getattr(
                self, "generated_html_file", self.config["output"]["html_file"]
            )
            logger.info(f"ğŸ“„ HTMLæŠ¥å‘Šå·²ç”Ÿæˆ: {html_file}")
        else:
            logger.warning("\nâš ï¸  éƒ¨åˆ†æ­¥éª¤å¤±è´¥ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—")


def main():
    """ä¸»å‡½æ•°"""
    print(
        """
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘     è‡ªåŠ¨åŒ–ä¸»é¢˜æ‘˜è¦é¡µé¢ç”Ÿæˆç³»ç»Ÿ                         â•‘
    â•‘     Automated Topic Summary Page Generation            â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """
    )

    pipeline = Pipeline()
    pipeline.run()

    print("\n" + "=" * 60)
    print("æ„Ÿè°¢ä½¿ç”¨ï¼")
    print("=" * 60)


if __name__ == "__main__":
    main()
