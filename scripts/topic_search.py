"""
äº¤äº’å¼ä¸»é¢˜æœç´¢çˆ¬è™«
åŠŸèƒ½ï¼šæ ¹æ®ç”¨æˆ·è¾“å…¥çš„ä¸»é¢˜å…³é”®è¯ï¼Œä½¿ç”¨ DeepSeek API æ™ºèƒ½æŸ¥æ‰¾ç›¸å…³æ–°é—»æºå¹¶çˆ¬å–
"""

import requests
from bs4 import BeautifulSoup
import json
import time
from datetime import datetime, timedelta
from typing import List, Dict
import logging
from urllib.parse import urljoin, quote
import os
import sys
from dotenv import load_dotenv

# æ·»åŠ  src åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(os.path.dirname(__file__)), 'src'))

load_dotenv()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class TopicScraper:
    """åŸºäºä¸»é¢˜çš„æ–°é—»çˆ¬è™«"""
    
    def __init__(self, topic: str, config_path: str = "config/config.json"):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            topic: æœç´¢ä¸»é¢˜/å…³é”®è¯
        """
        self.topic = topic
        self.articles = []
        
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = json.load(f)
        
        self.scraper_config = self.config['scraper']
        self.headers = {
            'User-Agent': self.scraper_config['user_agent']
        }
        
        # åˆå§‹åŒ–æ•°æ®åº“ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        self.use_database = True
        try:
            from scraper import DatabaseManager
            self.db_manager = DatabaseManager()
            logger.info("æ•°æ®åº“ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥ï¼Œå°†åªä¿å­˜åˆ°æ–‡ä»¶: {e}")
            self.use_database = False
            self.db_manager = None
        
        # åˆå§‹åŒ–æ™ºèƒ½æºæŸ¥æ‰¾å™¨ï¼ˆæ ¹æ®é…ç½®å†³å®šæ˜¯å¦å¯ç”¨ï¼‰
        topic_search_config = self.config.get('topic_search', {})
        self.use_intelligent_finder = topic_search_config.get('use_intelligent_finder', False)
        
        if self.use_intelligent_finder:
            try:
                from intelligent_source_finder import IntelligentSourceFinder
                self.source_finder = IntelligentSourceFinder()
                logger.info("æ™ºèƒ½æ–°é—»æºæŸ¥æ‰¾å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.warning(f"æ™ºèƒ½æºæŸ¥æ‰¾å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿæœç´¢: {e}")
                self.use_intelligent_finder = False
                self.source_finder = None
        else:
            logger.info("é…ç½®ä¸ºä½¿ç”¨ä¼ ç»Ÿæœç´¢å¼•æ“æ–¹æ³•ï¼ˆBing News RSSï¼‰")
            self.source_finder = None
    
    def search_baidu_news(self, max_results: int = 100) -> List[Dict]:
        """
        æœç´¢ç™¾åº¦æ–°é—»
        """
        articles = []
        search_url = f"https://www.baidu.com/s?tn=news&rtt=1&bsst=1&cl=2&wd={quote(self.topic)}"
        
        try:
            response = requests.get(search_url, headers=self.headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # æŸ¥æ‰¾æ–°é—»æ¡ç›®
            news_items = soup.find_all('div', class_='result', limit=max_results)
            
            for item in news_items:
                try:
                    # æå–æ ‡é¢˜å’Œé“¾æ¥
                    title_tag = item.find('h3') or item.find('a')
                    if title_tag:
                        link_tag = title_tag if title_tag.name == 'a' else title_tag.find('a')
                        if link_tag:
                            title = link_tag.get_text(strip=True)
                            url = link_tag.get('href', '')
                            
                            # æå–æ¥æº
                            source_tag = item.find('span', class_='c-color-gray')
                            source = source_tag.get_text(strip=True) if source_tag else 'ç™¾åº¦æ–°é—»'
                            
                            # æå–æ—¶é—´
                            time_tag = item.find('time')
                            published_date = datetime.now().strftime("%Y-%m-%d")
                            if time_tag and time_tag.get('datetime'):
                                try:
                                    published_date = time_tag.get('datetime').split('T')[0]
                                except:
                                    pass
                            
                            articles.append({
                                'title': title,
                                'url': url,
                                'source': source,
                                'published_date': published_date,
                                'content': '',  # éœ€è¦è¿›ä¸€æ­¥çˆ¬å–
                                'topic': self.topic,
                                'scraped_at': datetime.now().isoformat()
                            })
                except Exception as e:
                    logger.warning(f"è§£ææ–°é—»é¡¹å¤±è´¥: {e}")
                    continue
            
            logger.info(f"ä»ç™¾åº¦æ–°é—»æœç´¢åˆ° {len(articles)} ç¯‡å…³äº '{self.topic}' çš„æ–‡ç« ")
            
        except Exception as e:
            logger.error(f"æœç´¢ç™¾åº¦æ–°é—»å¤±è´¥: {e}")
        
        return articles
    
    def search_bing_news(self, max_results: int = 100) -> List[Dict]:
        """ä½¿ç”¨Bingæ–°é—»æœç´¢ï¼ˆRSS + ç½‘é¡µç‰ˆåˆ†é¡µï¼‰"""
        articles = []
        seen_urls = set()
        
        # å…ˆå°è¯•RSSè·å–æœ€æ–°çš„
        search_url = f"https://www.bing.com/news/search?q={quote(self.topic)}&format=rss"
        
        try:
            response = requests.get(search_url, headers=self.headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'xml')
            items = soup.find_all('item')
            
            for item in items:
                try:
                    title = item.find('title').get_text(strip=True) if item.find('title') else ''
                    url = item.find('link').get_text(strip=True) if item.find('link') else ''
                    description = item.find('description').get_text(strip=True) if item.find('description') else ''
                    
                    pub_date = item.find('pubDate')
                    published_date = datetime.now().strftime("%Y-%m-%d")
                    if pub_date:
                        try:
                            from dateutil import parser
                            dt = parser.parse(pub_date.get_text())
                            published_date = dt.strftime("%Y-%m-%d")
                        except:
                            pass
                    
                    # æå–æ¥æº
                    source_tag = item.find('source')
                    source = source_tag.get_text(strip=True) if source_tag else 'Bing News'
                    
                    if url not in seen_urls:
                        seen_urls.add(url)
                        articles.append({
                            'title': title,
                            'url': url,
                            'source': source,
                            'published_date': published_date,
                            'content': description,
                            'topic': self.topic,
                            'scraped_at': datetime.now().isoformat()
                        })
                except Exception as e:
                    logger.warning(f"è§£æBingæ–°é—»é¡¹å¤±è´¥: {e}")
                    continue
            
            logger.info(f"ä»Bing News RSSè·å– {len(articles)} ç¯‡æ–‡ç« ")
            
        except Exception as e:
            logger.error(f"æœç´¢Bing News RSSå¤±è´¥: {e}")
        
        # å¦‚æœéœ€è¦æ›´å¤šç»“æœï¼Œä½¿ç”¨ç½‘é¡µç‰ˆåˆ†é¡µæœç´¢
        if len(articles) < max_results:
            logger.info(f"RSSç»“æœä¸è¶³ï¼Œå°è¯•ç½‘é¡µç‰ˆè·å–æ›´å¤šï¼ˆç›®æ ‡: {max_results}ç¯‡ï¼‰")
            page = 1
            max_pages = min(10, (max_results // 10) + 1)  # æ¯é¡µçº¦10æ¡ç»“æœ
            
            while len(articles) < max_results and page <= max_pages:
                try:
                    # Bingæ–°é—»ç½‘é¡µç‰ˆURLï¼Œfirstå‚æ•°æ§åˆ¶åˆ†é¡µ
                    first_param = (page - 1) * 10 + 1
                    web_url = f"https://www.bing.com/news/search?q={quote(self.topic)}&first={first_param}"
                    
                    response = requests.get(web_url, headers=self.headers, timeout=10)
                    response.raise_for_status()
                    
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # æŸ¥æ‰¾æ–°é—»å¡ç‰‡
                    news_cards = soup.find_all('div', class_='news-card')
                    if not news_cards:
                        # å°è¯•å…¶ä»–å¯èƒ½çš„é€‰æ‹©å™¨
                        news_cards = soup.find_all('article')
                    
                    if not news_cards:
                        logger.warning(f"ç¬¬{page}é¡µæœªæ‰¾åˆ°æ–°é—»å¡ç‰‡ï¼Œåœæ­¢åˆ†é¡µ")
                        break
                    
                    page_articles = 0
                    for card in news_cards:
                        try:
                            # æå–æ ‡é¢˜å’Œé“¾æ¥
                            title_tag = card.find('a', class_='title')
                            if not title_tag:
                                title_tag = card.find('a')
                            
                            if not title_tag:
                                continue
                            
                            title = title_tag.get_text(strip=True)
                            url = title_tag.get('href', '')
                            
                            # è·³è¿‡å·²å­˜åœ¨çš„URL
                            if url in seen_urls or not url:
                                continue
                            
                            # æå–æè¿°
                            desc_tag = card.find('div', class_='snippet') or card.find('p')
                            description = desc_tag.get_text(strip=True) if desc_tag else ''
                            
                            # æå–æ¥æº
                            source_tag = card.find('span', class_='source')
                            source = source_tag.get_text(strip=True) if source_tag else 'Bing News'
                            
                            # æå–æ—¥æœŸ
                            date_tag = card.find('span', class_='time')
                            published_date = datetime.now().strftime("%Y-%m-%d")
                            if date_tag:
                                date_text = date_tag.get_text(strip=True)
                                # ç®€å•å¤„ç†æ—¥æœŸï¼ˆä»Šå¤©ã€æ˜¨å¤©ç­‰ï¼‰
                                if 'å°æ—¶' in date_text or 'åˆ†é’Ÿ' in date_text or 'åˆšåˆš' in date_text:
                                    published_date = datetime.now().strftime("%Y-%m-%d")
                                elif 'æ˜¨å¤©' in date_text:
                                    published_date = (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")
                            
                            seen_urls.add(url)
                            articles.append({
                                'title': title,
                                'url': url,
                                'source': source,
                                'published_date': published_date,
                                'content': description,
                                'topic': self.topic,
                                'scraped_at': datetime.now().isoformat()
                            })
                            page_articles += 1
                            
                            if len(articles) >= max_results:
                                break
                        
                        except Exception as e:
                            logger.warning(f"è§£æç½‘é¡µæ–°é—»å¡ç‰‡å¤±è´¥: {e}")
                            continue
                    
                    logger.info(f"ç¬¬{page}é¡µè·å– {page_articles} ç¯‡æ–‡ç« ï¼Œç´¯è®¡ {len(articles)} ç¯‡")
                    
                    if page_articles == 0:
                        logger.warning(f"ç¬¬{page}é¡µæ— æ–°æ–‡ç« ï¼Œåœæ­¢åˆ†é¡µ")
                        break
                    
                    page += 1
                    time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
                
                except Exception as e:
                    logger.error(f"æœç´¢Bing Newsç½‘é¡µç‰ˆç¬¬{page}é¡µå¤±è´¥: {e}")
                    break
            
            logger.info(f"ä»Bing Newså…±è·å– {len(articles)} ç¯‡å…³äº '{self.topic}' çš„æ–‡ç« ")
        
        return articles[:max_results]
    
    def scrape_article_content(self, url: str) -> str:
        """çˆ¬å–æ–‡ç« è¯¦ç»†å†…å®¹"""
        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # æå–å†…å®¹
            content = ""
            for selector in ['article', '.article-body', '.story-body', '.content', 'main']:
                content_tag = soup.select_one(selector)
                if content_tag:
                    paragraphs = content_tag.find_all('p')
                    content = ' '.join([p.get_text(strip=True) for p in paragraphs])
                    break
            
            if not content:
                paragraphs = soup.find_all('p')
                content = ' '.join([p.get_text(strip=True) for p in paragraphs[:10]])
            
            return content[:5000]  # é™åˆ¶é•¿åº¦
            
        except Exception as e:
            logger.warning(f"çˆ¬å–æ–‡ç« å†…å®¹å¤±è´¥ {url}: {e}")
            return ""
    
    def search_with_intelligent_finder(self, max_results: int = 100) -> List[Dict]:
        """ä½¿ç”¨æ™ºèƒ½æºæŸ¥æ‰¾å™¨æœç´¢æ–°é—»"""
        logger.info(f"ä½¿ç”¨ DeepSeek API æ™ºèƒ½æŸ¥æ‰¾ä¸»é¢˜: '{self.topic}' çš„æ–°é—»æº")
        
        all_articles = []
        
        try:
            # 1. è·å–æ¨èçš„æ–°é—»æº
            sources = self.source_finder.find_news_sources(self.topic, max_sources=5)
            logger.info(f"DeepSeek æ¨èäº† {len(sources)} ä¸ªæ–°é—»æº")
            
            for source in sources:
                print(f"\nâœ“ æ¨èæº: {source['name']} - {source.get('description', '')}")
                print(f"  URL: {source['url']}")
            
            # 2. ä»æ¯ä¸ªæ¨èçš„æºçˆ¬å–æ–°é—»
            for source in sources:
                try:
                    logger.info(f"æ­£åœ¨ä» {source['name']} çˆ¬å–æ–°é—»...")
                    
                    # å°è¯•ä»æºçš„ä¸»é¡µçˆ¬å–
                    articles = self.scrape_from_source(
                        source['url'], 
                        source['name'],
                        max_articles=max_results // len(sources)
                    )
                    
                    all_articles.extend(articles)
                    logger.info(f"ä» {source['name']} è·å–äº† {len(articles)} ç¯‡æ–‡ç« ")
                    
                    time.sleep(2)  # å»¶è¿Ÿé¿å…è¿‡å¿«è¯·æ±‚
                    
                except Exception as e:
                    logger.error(f"ä» {source['name']} çˆ¬å–å¤±è´¥: {e}")
                    continue
            
        except Exception as e:
            logger.error(f"æ™ºèƒ½æºæŸ¥æ‰¾å¤±è´¥: {e}")
            # å›é€€åˆ°ä¼ ç»Ÿæœç´¢
            logger.info("å›é€€åˆ°ä¼ ç»Ÿæœç´¢æ–¹æ³•...")
            return self.search_topic_traditional(max_results)
        
        # å»é‡
        seen_urls = set()
        unique_articles = []
        for article in all_articles:
            if article['url'] not in seen_urls:
                seen_urls.add(article['url'])
                unique_articles.append(article)
        
        logger.info(f"å»é‡åå…± {len(unique_articles)} ç¯‡æ–‡ç« ")
        self.articles = unique_articles
        return unique_articles
    
    def scrape_from_source(self, source_url: str, source_name: str, max_articles: int = 5) -> List[Dict]:
        """ä»æŒ‡å®šæ–°é—»æºçˆ¬å–æ–‡ç« """
        articles = []
        
        try:
            response = requests.get(source_url, headers=self.headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # æŸ¥æ‰¾æ–‡ç« é“¾æ¥ï¼ˆé€šç”¨æ–¹æ³•ï¼‰
            article_links = []
            
            # æ–¹æ³•1: æŸ¥æ‰¾åŒ…å«ä¸»é¢˜å…³é”®è¯çš„é“¾æ¥
            for a_tag in soup.find_all('a', href=True):
                title = a_tag.get_text(strip=True)
                href = a_tag.get('href')
                
                # æ£€æŸ¥æ˜¯å¦ç›¸å…³
                if title and len(title) > 10 and self.topic.lower() in title.lower():
                    full_url = urljoin(source_url, href)
                    if full_url.startswith('http'):
                        article_links.append({
                            'title': title,
                            'url': full_url,
                            'source': source_name
                        })
                        
                        if len(article_links) >= max_articles:
                            break
            
            # æ–¹æ³•2: å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•çˆ¬å–æœ€æ–°æ–‡ç« 
            if not article_links:
                logger.info(f"æœªæ‰¾åˆ°åŒ…å«å…³é”®è¯çš„æ–‡ç« ï¼Œçˆ¬å– {source_name} æœ€æ–°æ–‡ç« ...")
                for a_tag in soup.find_all('a', href=True, limit=max_articles * 2):
                    title = a_tag.get_text(strip=True)
                    href = a_tag.get('href')
                    
                    if title and len(title) > 20 and len(title) < 200:
                        full_url = urljoin(source_url, href)
                        if full_url.startswith('http') and 'article' in full_url or 'news' in full_url:
                            article_links.append({
                                'title': title,
                                'url': full_url,
                                'source': source_name
                            })
                            
                            if len(article_links) >= max_articles:
                                break
            
            # çˆ¬å–æ–‡ç« å†…å®¹
            for article_info in article_links[:max_articles]:
                try:
                    time.sleep(1)
                    content = self.scrape_article_content(article_info['url'])
                    
                    articles.append({
                        'title': article_info['title'],
                        'url': article_info['url'],
                        'source': article_info['source'],
                        'published_date': datetime.now().strftime("%Y-%m-%d"),
                        'content': content,
                        'scraped_at': datetime.now().isoformat()
                    })
                    
                except Exception as e:
                    logger.warning(f"çˆ¬å–æ–‡ç« å¤±è´¥ {article_info['url']}: {e}")
                    continue
            
        except Exception as e:
            logger.error(f"ä» {source_url} çˆ¬å–å¤±è´¥: {e}")
        
        return articles
    
    def search_topic_traditional(self, max_results: int = 100) -> List[Dict]:
        """ä¼ ç»Ÿæœç´¢æ–¹æ³•ï¼ˆä½¿ç”¨ Bing/Googleï¼‰"""
        logger.info(f"ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•æœç´¢ä¸»é¢˜: '{self.topic}'")
        
        all_articles = []
        
        # å°è¯•å¤šä¸ªæœç´¢æº
        sources = [
            ('Bing News', self.search_bing_news),
            # ç™¾åº¦æ–°é—»æœ‰åçˆ¬è™«éªŒè¯ï¼Œæš‚æ—¶ç¦ç”¨
            # ('ç™¾åº¦æ–°é—»', self.search_baidu_news)
        ]
        
        for source_name, search_func in sources:
            try:
                logger.info(f"ä» {source_name} æœç´¢...")
                articles = search_func(max_results=max_results // len(sources))
                
                # çˆ¬å–æ–‡ç« è¯¦ç»†å†…å®¹
                for article in articles:
                    if not article.get('content'):
                        time.sleep(1)
                        article['content'] = self.scrape_article_content(article['url'])
                
                all_articles.extend(articles)
                logger.info(f"ä» {source_name} è·å–äº† {len(articles)} ç¯‡æ–‡ç« ")
                
                time.sleep(2)
                
            except Exception as e:
                logger.error(f"ä» {source_name} æœç´¢å¤±è´¥: {e}")
                continue
        
        # å»é‡
        seen_urls = set()
        unique_articles = []
        for article in all_articles:
            if article['url'] not in seen_urls:
                seen_urls.add(article['url'])
                unique_articles.append(article)
        
        logger.info(f"å»é‡åå…± {len(unique_articles)} ç¯‡æ–‡ç« ")
        self.articles = unique_articles
        return unique_articles
    
    def search_topic(self, max_results: int = 100) -> List[Dict]:
        """æœç´¢ç‰¹å®šä¸»é¢˜çš„æ–°é—»ï¼ˆæ™ºèƒ½é€‰æ‹©æ–¹æ³•ï¼‰"""
        if self.use_intelligent_finder and self.source_finder:
            return self.search_with_intelligent_finder(max_results)
        else:
            return self.search_topic_traditional(max_results)
        return unique_articles
    
    def save_to_file(self, output_path: str = None):
        """ä¿å­˜åˆ°æ–‡ä»¶"""
        if output_path is None:
            # ä½¿ç”¨ä¸»é¢˜åç§°åˆ›å»ºæ–‡ä»¶å
            safe_topic = "".join(c for c in self.topic if c.isalnum() or c in (' ', '-', '_')).strip()
            safe_topic = safe_topic.replace(' ', '_')
            output_path = f"data/topic_{safe_topic}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        data = {
            'topic': self.topic,
            'article_count': len(self.articles),
            'scraped_at': datetime.now().isoformat(),
            'articles': self.articles
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æ•°æ®å·²ä¿å­˜åˆ° {output_path}")
        return output_path
    
    def save_to_database(self):
        """ä¿å­˜åˆ°æ•°æ®åº“"""
        if not self.use_database or not self.db_manager:
            logger.warning("æ•°æ®åº“æœªå¯ç”¨")
            return 0
        
        return self.db_manager.insert_articles_batch(self.articles)
    
    def close(self):
        """å…³é—­èµ„æº"""
        if self.db_manager:
            self.db_manager.close()


def interactive_search():
    """äº¤äº’å¼æœç´¢ç•Œé¢"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         ä¸»é¢˜æ–°é—»æœç´¢ç³»ç»Ÿ                               â•‘
â•‘         Topic News Search System                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # è·å–ç”¨æˆ·è¾“å…¥
    print("è¯·è¾“å…¥æ‚¨æƒ³æœç´¢çš„ä¸»é¢˜æˆ–äº‹ä»¶ï¼ˆä¾‹å¦‚ï¼šäººå·¥æ™ºèƒ½ã€æ°”å€™å˜åŒ–ã€ç§‘æŠ€æ–°é—»ç­‰ï¼‰")
    topic = input("\nğŸ” æœç´¢ä¸»é¢˜: ").strip()
    
    if not topic:
        print("âŒ ä¸»é¢˜ä¸èƒ½ä¸ºç©ºï¼")
        return
    
    print(f"\nâœ“ æœç´¢ä¸»é¢˜: {topic}")
    
    # è¯¢é—®æœç´¢æ•°é‡
    try:
        max_results = input("\nğŸ“Š éœ€è¦æœç´¢å¤šå°‘ç¯‡æ–‡ç« ï¼Ÿ(é»˜è®¤100): ").strip()
        max_results = int(max_results) if max_results else 100
    except:
        max_results = 100
    
    print(f"âœ“ æœç´¢æ•°é‡: {max_results}")
    
    # è¯¢é—®æ˜¯å¦çˆ¬å–è¯¦ç»†å†…å®¹
    crawl_content = input("\nğŸ“° æ˜¯å¦çˆ¬å–æ–‡ç« è¯¦ç»†å†…å®¹ï¼Ÿ(y/n, é»˜è®¤nï¼Œè€—æ—¶è¾ƒé•¿): ").strip().lower()
    crawl_content = crawl_content == 'y'
    
    print("\n" + "=" * 60)
    print("å¼€å§‹æœç´¢...")
    print("=" * 60 + "\n")
    
    # æ‰§è¡Œæœç´¢
    scraper = TopicScraper(topic)
    
    try:
        articles = scraper.search_topic(max_results=max_results)
        
        if not articles:
            print("\nâŒ æœªæ‰¾åˆ°ç›¸å…³æ–‡ç« ")
            return
        
        print(f"\nâœ“ æ‰¾åˆ° {len(articles)} ç¯‡ç›¸å…³æ–‡ç« \n")
        
        # æ˜¾ç¤ºå‰5ç¯‡
        print("=" * 60)
        print("æ–‡ç« é¢„è§ˆï¼ˆå‰5ç¯‡ï¼‰:")
        print("=" * 60)
        for i, article in enumerate(articles[:5], 1):
            print(f"\n{i}. ã€{article['source']}ã€‘{article['title']}")
            print(f"   URL: {article['url'][:80]}...")
            print(f"   æ—¥æœŸ: {article['published_date']}")
        
        # ä¿å­˜é€‰é¡¹
        print("\n" + "=" * 60)
        save_option = input("\nğŸ’¾ ä¿å­˜é€‰é¡¹:\n  1. ä»…ä¿å­˜åˆ°æ–‡ä»¶\n  2. ä¿å­˜åˆ°æ–‡ä»¶å’Œæ•°æ®åº“\n  3. ä¸ä¿å­˜\n\nè¯·é€‰æ‹© (1/2/3, é»˜è®¤1): ").strip()
        
        if save_option == '3':
            print("\nâœ“ æ•°æ®æœªä¿å­˜")
        else:
            # ä¿å­˜åˆ°æ–‡ä»¶
            file_path = scraper.save_to_file()
            print(f"\nâœ“ å·²ä¿å­˜åˆ°æ–‡ä»¶: {file_path}")
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            if save_option == '2':
                db_count = scraper.save_to_database()
                print(f"âœ“ å·²ä¿å­˜åˆ°æ•°æ®åº“: {db_count} ç¯‡æ–‡ç« ")
        
        # è¯¢é—®æ˜¯å¦ç»§ç»­å¤„ç†
        print("\n" + "=" * 60)
        continue_process = input("\næ˜¯å¦ç»§ç»­å¤„ç†è¿™äº›æ•°æ®ï¼ˆç”Ÿæˆæ‘˜è¦ã€å®ä½“æå–ç­‰ï¼‰ï¼Ÿ(y/n, é»˜è®¤n): ").strip().lower()
        
        if continue_process == 'y':
            # å°†æ•°æ®ä¿å­˜ä¸ºæ ‡å‡†æ ¼å¼ä¾›åç»­å¤„ç†
            standard_file = "data/raw_articles.json"
            with open(standard_file, 'w', encoding='utf-8') as f:
                json.dump(articles, f, ensure_ascii=False, indent=2)
            
            print(f"\nâœ“ æ•°æ®å·²å‡†å¤‡å®Œæˆ")
            print("\nç°åœ¨å¯ä»¥è¿è¡Œä»¥ä¸‹å‘½ä»¤ç»§ç»­å¤„ç†:")
            print("  python main.py        # è¿è¡Œå®Œæ•´çš„å¤„ç†ç®¡é“")
            print("  python demo.py        # è¿è¡Œæ¼”ç¤ºç¨‹åº")
        
    finally:
        scraper.close()
    
    print("\n" + "=" * 60)
    print("æœç´¢å®Œæˆï¼")
    print("=" * 60)


if __name__ == "__main__":
    interactive_search()
